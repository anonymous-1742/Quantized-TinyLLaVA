{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a409371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClientModel(TinyLlavaPreTrainedModel):\n",
    "    def __init__(self,config:TinyLlavaConfig,llm,tokenizer):\n",
    "        self.llm_embedding =llm.get_input_embeddings()\n",
    "        self.vision_tower = VisionTowerFactory(config.vision_model_name_or_path)(config.vision_config)\n",
    "        self.connector = ConnectorFactory(config.connector_type)(config)\n",
    "        self.tokenizer = post_load(Tokenizer.from_pretrained(\n",
    "            config.tokenizer_name_or_path,\n",
    "            cache_dir = config.cache_dir,\n",
    "            model_max_length = config.tokenizer_model_max_length,\n",
    "            padding_side = config.tokenizer_padding_side,\n",
    "            use_fast = config.tokenizer_use_fast,\n",
    "        ))\n",
    "        self.post_init()\n",
    "    def get_input_embeddings(self):\n",
    "        return self.language_model.get_input_embeddings()\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.language_model.set_input_embeddings(value)\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.language_model.get_output_embeddings()\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.language_model.set_output_embeddings(new_embeddings)\n",
    "\n",
    "    def set_decoder(self, decoder):\n",
    "        self.language_model.set_decoder(decoder)\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.language_model.get_decoder()\n",
    "\n",
    "    def tie_weights(self):\n",
    "        return self.language_model.tie_weights()\n",
    "\n",
    "    def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of=None) -> nn.Embedding:\n",
    "        model_embeds = self.language_model.resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n",
    "        # update vocab size\n",
    "        self.config.text_config.vocab_size = model_embeds.num_embeddings\n",
    "        self.config.vocab_size = model_embeds.num_embeddings\n",
    "        self.vocab_size = model_embeds.num_embeddings\n",
    "        return model_embeds\n",
    "\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        images: Optional[torch.FloatTensor] = None,\n",
    "        image_sizes: Optional[List[List[int]]] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        if inputs_embeds is None:\n",
    "            (\n",
    "                input_ids,\n",
    "                position_ids,\n",
    "                attention_mask,\n",
    "                past_key_values,\n",
    "                inputs_embeds,\n",
    "                labels\n",
    "            ) = self.prepare_inputs_labels_for_multimodal(\n",
    "                input_ids,\n",
    "                position_ids,\n",
    "                attention_mask,\n",
    "                past_key_values,\n",
    "                labels,\n",
    "                images,\n",
    "                image_sizes\n",
    "            )\n",
    "        return (input_ids,\n",
    "                position_ids,\n",
    "                attention_mask,\n",
    "                past_key_values,\n",
    "                inputs_embeds,\n",
    "                labels)\n",
    "    \n",
    "\n",
    "    def encode_images(self, images):\n",
    "        kwargs = {}\n",
    "        kwargs['vision_feature_layer'] = self.config.vision_feature_layer\n",
    "        kwargs['vision_feature_select_strategy'] = self.config.vision_feature_select_strategy\n",
    "        images = images.to(device=self.device, dtype=self.dtype)\n",
    "        image_features = self.vision_tower(images, **kwargs)\n",
    "        image_features = self.connector(image_features)\n",
    "        return image_features\n",
    "    \n",
    "    def prepare_inputs_labels_for_multimodal(\n",
    "        self, input_ids, position_ids, attention_mask, past_key_values, labels,\n",
    "        images, image_sizes=None\n",
    "    ):\n",
    "        vision_tower = self.vision_tower\n",
    "        if vision_tower is None or images is None or input_ids.shape[1] == 1:\n",
    "            return input_ids, position_ids, attention_mask, past_key_values, None, labels\n",
    "\n",
    "        \n",
    "        image_features = self.encode_images(images)\n",
    "\n",
    "        # TODO: image start / end is not implemented here to support pretraining.\n",
    "        if getattr(self.config, 'tune_mm_mlp_adapter', False):\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Let's just add dummy tensors if they do not exist,\n",
    "        # it is a headache to deal with None all the time.\n",
    "        # But it is not ideal, and if you have a better idea,\n",
    "        # please open an issue / submit a PR, thanks.\n",
    "        _labels = labels\n",
    "        _position_ids = position_ids\n",
    "        _attention_mask = attention_mask\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\n",
    "        else:\n",
    "            attention_mask = attention_mask.bool()\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\n",
    "        if labels is None:\n",
    "            labels = torch.full_like(input_ids, IGNORE_INDEX)\n",
    "\n",
    "        # remove the padding using attention_mask -- FIXME\n",
    "        _input_ids = input_ids\n",
    "        input_ids = [cur_input_ids[cur_attention_mask] for cur_input_ids, cur_attention_mask in zip(input_ids, attention_mask)]\n",
    "        labels = [cur_labels[cur_attention_mask] for cur_labels, cur_attention_mask in zip(labels, attention_mask)]\n",
    "\n",
    "        new_input_embeds = []\n",
    "        new_labels = []\n",
    "        cur_image_idx = 0\n",
    "        for batch_idx, cur_input_ids in enumerate(input_ids):\n",
    "            num_images = (cur_input_ids == IMAGE_TOKEN_INDEX).sum()\n",
    "            if num_images == 0:\n",
    "                cur_image_features = image_features[cur_image_idx]\n",
    "                cur_input_embeds_1 = self.language_model.get_input_embeddings()(cur_input_ids)\n",
    "                cur_input_embeds = torch.cat([cur_input_embeds_1, cur_image_features[0:0]], dim=0)\n",
    "                new_input_embeds.append(cur_input_embeds)\n",
    "                new_labels.append(labels[batch_idx])\n",
    "                cur_image_idx += 1\n",
    "                continue\n",
    "\n",
    "            image_token_indices = [-1] + torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0].tolist() + [cur_input_ids.shape[0]]\n",
    "            cur_input_ids_noim = []\n",
    "            cur_labels = labels[batch_idx]\n",
    "            cur_labels_noim = []\n",
    "            for i in range(len(image_token_indices) - 1):\n",
    "                cur_input_ids_noim.append(cur_input_ids[image_token_indices[i]+1:image_token_indices[i+1]])\n",
    "                cur_labels_noim.append(cur_labels[image_token_indices[i]+1:image_token_indices[i+1]])\n",
    "            split_sizes = [x.shape[0] for x in cur_labels_noim]\n",
    "            cur_input_embeds = self.language_model.get_input_embeddings()(torch.cat(cur_input_ids_noim))\n",
    "            cur_input_embeds_no_im = torch.split(cur_input_embeds, split_sizes, dim=0)\n",
    "            cur_new_input_embeds = []\n",
    "            cur_new_labels = []\n",
    "\n",
    "            for i in range(num_images + 1):\n",
    "                cur_new_input_embeds.append(cur_input_embeds_no_im[i])\n",
    "                cur_new_labels.append(cur_labels_noim[i])\n",
    "                if i < num_images:\n",
    "                    cur_image_features = image_features[cur_image_idx]\n",
    "                    cur_image_idx += 1\n",
    "                    cur_new_input_embeds.append(cur_image_features)\n",
    "                    cur_new_labels.append(torch.full((cur_image_features.shape[0],), IGNORE_INDEX, device=cur_labels.device, dtype=cur_labels.dtype))\n",
    "\n",
    "            cur_new_input_embeds = [x.to(self.device) for x in cur_new_input_embeds]\n",
    "\n",
    "            cur_new_input_embeds = torch.cat(cur_new_input_embeds)\n",
    "            cur_new_labels = torch.cat(cur_new_labels)\n",
    "\n",
    "            new_input_embeds.append(cur_new_input_embeds)\n",
    "            new_labels.append(cur_new_labels)\n",
    "\n",
    "        # Truncate sequences to max length as image embeddings can make the sequence longer\n",
    "        tokenizer_model_max_length = getattr(self.config, 'tokenizer_model_max_length', None)\n",
    "        if tokenizer_model_max_length is not None:\n",
    "            new_input_embeds = [x[:tokenizer_model_max_length] for x in new_input_embeds]\n",
    "            new_labels = [x[:tokenizer_model_max_length] for x in new_labels]\n",
    "\n",
    "        # Combine them\n",
    "        max_len = max(x.shape[0] for x in new_input_embeds)\n",
    "        batch_size = len(new_input_embeds)\n",
    "\n",
    "        new_input_embeds_padded = []\n",
    "        new_labels_padded = torch.full((batch_size, max_len), IGNORE_INDEX, dtype=new_labels[0].dtype, device=new_labels[0].device)\n",
    "        attention_mask = torch.zeros((batch_size, max_len), dtype=attention_mask.dtype, device=attention_mask.device)\n",
    "        position_ids = torch.zeros((batch_size, max_len), dtype=position_ids.dtype, device=position_ids.device)\n",
    "\n",
    "        for i, (cur_new_embed, cur_new_labels) in enumerate(zip(new_input_embeds, new_labels)):\n",
    "            cur_len = cur_new_embed.shape[0]\n",
    "            if getattr(self.config, 'tokenizer_padding_side', 'right') == \"left\":\n",
    "                new_input_embeds_padded.append(torch.cat((\n",
    "                    torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device),\n",
    "                    cur_new_embed\n",
    "                ), dim=0))\n",
    "                if cur_len > 0:\n",
    "                    new_labels_padded[i, -cur_len:] = cur_new_labels\n",
    "                    attention_mask[i, -cur_len:] = True\n",
    "                    position_ids[i, -cur_len:] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n",
    "            else:\n",
    "                new_input_embeds_padded.append(torch.cat((\n",
    "                    cur_new_embed,\n",
    "                    torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device)\n",
    "                ), dim=0))\n",
    "                if cur_len > 0:\n",
    "                    new_labels_padded[i, :cur_len] = cur_new_labels\n",
    "                    attention_mask[i, :cur_len] = True\n",
    "                    position_ids[i, :cur_len] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n",
    "\n",
    "        new_input_embeds = torch.stack(new_input_embeds_padded, dim=0)\n",
    "\n",
    "        if _labels is None:\n",
    "            new_labels = None\n",
    "        else:\n",
    "            new_labels = new_labels_padded\n",
    "\n",
    "        if _attention_mask is None:\n",
    "            attention_mask = None\n",
    "        else:\n",
    "            attention_mask = attention_mask.to(dtype=_attention_mask.dtype)\n",
    "\n",
    "        if _position_ids is None:\n",
    "            position_ids = None\n",
    "\n",
    "        return None, position_ids, attention_mask, past_key_values, new_input_embeds, new_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caec5cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ServerModel(TinyLlavaPreTrainedModel):\n",
    "    def __init__(self,config:TinyLlavaConfig,llm):\n",
    "        self.llm_ =llm\n",
    "        self.post_init()\n",
    "        \n",
    "    def set_input_embeddings(self, value):\n",
    "        self.language_model.set_input_embeddings(value)\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.language_model.get_output_embeddings()\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.language_model.set_output_embeddings(new_embeddings)\n",
    "\n",
    "    def set_decoder(self, decoder):\n",
    "        self.language_model.set_decoder(decoder)\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.language_model.get_decoder()\n",
    "\n",
    "    def tie_weights(self):\n",
    "        return self.language_model.tie_weights()\n",
    "\n",
    "    def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of=None) -> nn.Embedding:\n",
    "        model_embeds = self.language_model.resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n",
    "        # update vocab size\n",
    "        self.config.text_config.vocab_size = model_embeds.num_embeddings\n",
    "        self.config.vocab_size = model_embeds.num_embeddings\n",
    "        self.vocab_size = model_embeds.num_embeddings\n",
    "        return model_embeds\n",
    "\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        if inputs_embeds is None:\n",
    "            (\n",
    "                input_ids,\n",
    "                position_ids,\n",
    "                attention_mask,\n",
    "                past_key_values,\n",
    "                inputs_embeds,\n",
    "                labels\n",
    "            ) = self.prepare_inputs_labels_for_multimodal(\n",
    "                input_ids,\n",
    "                position_ids,\n",
    "                attention_mask,\n",
    "                past_key_values,\n",
    "                labels,\n",
    "                images,\n",
    "                image_sizes\n",
    "            )\n",
    "        return self.language_model.forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            labels=labels,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        inputs: Optional[torch.Tensor] = None,\n",
    "        images: Optional[torch.Tensor] = None,\n",
    "        image_sizes: Optional[torch.Tensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> Union[GenerateOutput, torch.LongTensor]:\n",
    "        position_ids = kwargs.pop(\"position_ids\", None)\n",
    "        attention_mask = kwargs.pop(\"attention_mask\", None)\n",
    "        if \"inputs_embeds\" in kwargs:\n",
    "            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n",
    "\n",
    "        if images is not None:\n",
    "            (\n",
    "                inputs,\n",
    "                position_ids,\n",
    "                attention_mask,\n",
    "                _,\n",
    "                inputs_embeds,\n",
    "                _\n",
    "            ) = self.prepare_inputs_labels_for_multimodal(\n",
    "                inputs,\n",
    "                position_ids,\n",
    "                attention_mask,\n",
    "                None,\n",
    "                None,\n",
    "                images,\n",
    "                image_sizes=image_sizes\n",
    "            )\n",
    "        else:\n",
    "            inputs_embeds = self.language_model.get_input_embeddings()(inputs)\n",
    "\n",
    "        return self.language_model.generate(\n",
    "            position_ids=position_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    def prepare_inputs_for_generation(self, input_ids, past_key_values=None,\n",
    "                                      inputs_embeds=None, **kwargs):\n",
    "        images = kwargs.pop(\"images\", None)\n",
    "        image_sizes = kwargs.pop(\"image_sizes\", None)\n",
    "        inputs = self.language_model.prepare_inputs_for_generation(\n",
    "            input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs\n",
    "        )\n",
    "        if images is not None:\n",
    "            inputs['images'] = images\n",
    "        if image_sizes is not None:\n",
    "            inputs['image_sizes'] = image_sizes\n",
    "        return inputs\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eac831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_client_server_models(config:TinyLlavaConfig):\n",
    "    llm=LLMFactory(config.llm_model_name_or_path)[0]\n",
    "    (Tokenizer, post_load) = LLMFactory(config.llm_model_name_or_path)[1]\n",
    "    tokenizer = post_load(Tokenizer.from_pretrained(\n",
    "            config.tokenizer_name_or_path,\n",
    "            cache_dir = config.cache_dir,\n",
    "            model_max_length = config.tokenizer_model_max_length,\n",
    "            padding_side = config.tokenizer_padding_side,\n",
    "            use_fast = config.tokenizer_use_fast,\n",
    "        ))\n",
    "    client=ClientModel(config_TinyLlavaConfig,llm,tokenizer)\n",
    "    server=ServerModel(config_TinyLlavaConfig,llm)\n",
    "    return client,server\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
