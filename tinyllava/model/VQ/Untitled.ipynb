{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b452a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "# ==========================================================\n",
    "# 基础量化器：Two-stage & Mean-value\n",
    "# ==========================================================\n",
    "\n",
    "def uniform_quantizer(x,qmin,qmax,Q):\n",
    "    x=np.clip(x,qmin,qmax)\n",
    "    indices=np.around((x-qmin)/(qmax-qmin)*(Q-1))\n",
    "    return(indices,qmin,qmax)\n",
    "\n",
    "def two_stage_quantizer(a,Q_ep=16,Q_entry=64):\n",
    "    a_min= np.min(a,axis=0)\n",
    "    a_max= np.max(a,axis=0)\n",
    "    min_indices,min_low,min_up=uniform_quantizer(a_min,np.min(a_min),np.max(a_min),Q_ep)\n",
    "    max_indices,max_low,max_up=uniform_quantizer(a_max,np.min(a_max),np.max(a_max),Q_ep)\n",
    "    min_q=min_indices/(Q_ep-1)*(min_up-min_low)+min_low\n",
    "    max_q=max_indices/(Q_ep-1)*(max_up-max_low)+max_low\n",
    "    entry_indices,entry_low,entry_up=uniform_quantizer(a,min_q,max_q,Q_entry)\n",
    "    Q=entry_indices/Q_entry*(entry_up-entry_low)+entry_low\n",
    "    return(Q)\n",
    "\n",
    "\n",
    "def mean_value_quantizer(a, Q0=8):\n",
    "    \"\"\"均值量化器 (Mean-value quantizer)\"\"\"\n",
    "    a_mean = np.mean(a,axis=0)\n",
    "    mean_indices,mean_low,mean_up=uniform_quantizer(a_mean,np.min(a_mean),np.max(a_mean),Q0)\n",
    "    mean_q=mean_indices/Q0*(mean_up-mean_low)+mean_low\n",
    "    a_q = np.repeat(mean_q.reshape(1, -1),a.shape[0], axis=0)\n",
    "    return a_q,mean_q\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# (P) 问题求解：Water-filling 分配 Q_i\n",
    "# ==========================================================\n",
    "def solve_quantization_levels(a_tilde,a_tilde_0, B, Cava, tol=1e-4, max_iter=100):\n",
    "    \"\"\"\n",
    "    求解 (P): 最优 Q_i 分配（基于 KKT 条件的 water-filling）\n",
    "    a_tilde: 向量 range 数组\n",
    "    B: batch size\n",
    "    Cava: 可用通信预算 (bit)\n",
    "    \"\"\"\n",
    "    M = len(a_tilde)\n",
    "    nu_low, nu_high = 1e-12, 1e6\n",
    "\n",
    "    def compute_Q(nu):\n",
    "        u = (a_tilde**2 * np.log(2)) / (2 * nu)\n",
    "        u0=(a_tilde_0**2*B* np.log(2)) / (nu)\n",
    "        u=np.insert(u,0,u0)\n",
    "        v = (u * np.sqrt(81 - 12*u) + 9*u) ** (1/3)\n",
    "        Q = ((2/3)**(1/3)) * (u / v) + v / (2**(1/3) * 3**(2/3)) + 1\n",
    "        Q = np.clip(Q, 2, 2**32)\n",
    "\n",
    "        return Q\n",
    "    for _ in range(max_iter):\n",
    "        nu_mid = (nu_low + nu_high) / 2\n",
    "        Q_mid = compute_Q(nu_mid)\n",
    "        bit_sum = np.sum(np.log2(Q_mid))  # 总通信开销近似\n",
    "        if bit_sum > Cava:  # 超出预算 -> 增大 ν\n",
    "            nu_low = nu_mid\n",
    "        else:\n",
    "            nu_high = nu_mid\n",
    "        if abs(bit_sum - Cava) < tol:\n",
    "            break\n",
    "\n",
    "    return compute_Q(nu_mid), nu_mid\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 自动确定 M* ：搜索最优 M 来最小化量化误差\n",
    "# ==========================================================\n",
    "def auto_determine_M_and_Q(a_ranges,a_tilde_0, B, D_hat, Cava):\n",
    "    \"\"\"\n",
    "    自动搜索最优 M* 并为其求解 (P)\n",
    "    \"\"\"\n",
    "    candidates = np.unique(np.linspace(1, D_hat // 2, num=8, dtype=int))\n",
    "    best_M, best_Q, best_err = 0, None, np.inf\n",
    "    for M in candidates:\n",
    "        a_tilde = a_ranges[:M]\n",
    "        a_bar=a_ranges[M:]\n",
    "        Q_all, _ = solve_quantization_levels(a_tilde,a_tilde_0, B, Cava)\n",
    "        Q0=Q_all[0]\n",
    "        Q_entry=Q_all[1:]\n",
    "        # 计算误差上界 (式19)\n",
    "        err_two = np.sum((a_tilde**2 * B) / (4 * (Q_entry - 1)**2))\n",
    "        err_mean_1 = np.sum((a_bar**2 * B)/2) # mean-quantizer误差近似\n",
    "        err_mean_2=a_tilde_0**2*B*(D_hat-M)/(2*(Q0 - 1)**2)\n",
    "        total_err = err_two + err_mean_1+err_mean_2\n",
    "        if total_err < best_err:\n",
    "            best_err = total_err\n",
    "            best_M = M\n",
    "            best_Q = Q_all\n",
    "\n",
    "    return best_M, best_Q\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 主算法：Adaptive Feature-Wise Quantization\n",
    "# ==========================================================\n",
    "def adaptive_featurewise_quantization(A, Cava, Q_ep=200):\n",
    "    \"\"\"\n",
    "    自适应特征量化算法（完整版本）\n",
    "    输入：\n",
    "        A: (B×D̂) 中间特征矩阵\n",
    "        Cava: 总通信预算（bit）\n",
    "    输出：\n",
    "        Q: 量化后矩阵\n",
    "        mu: 均值量化向量\n",
    "        M*: 使用 two-stage quantizer 的列数\n",
    "        Q_entry_list: 对应的量化级数组\n",
    "    \"\"\"\n",
    "    B, D_hat = A.shape\n",
    "    A=A\n",
    "    ranges = np.max(A, axis=0) - np.min(A, axis=0)\n",
    "    idx_sorted = np.argsort(-ranges)  # 按range从大到小排序\n",
    "    A_sorted = A[:, idx_sorted]\n",
    "    ranges_sorted = ranges[idx_sorted]\n",
    "    a_tilde_0=np.max(np.mean(A,axis=0))-np.min(np.mean(A,axis=0))\n",
    "    # ① 自动确定 M* 和每列最优 Q_i (P问题)\n",
    "    M_star, Q_entry_list = auto_determine_M_and_Q(ranges_sorted,a_tilde_0, B, D_hat, Cava)\n",
    "    # ② 执行量化\n",
    "    Q = np.zeros_like(A_sorted)\n",
    "    mu = np.zeros(D_hat)\n",
    "    Q[:,:M_star]=two_stage_quantizer(A_sorted[:, 0:M_star], Q_ep=Q_ep,Q_entry=np.around(Q_entry_list[1:]))\n",
    "    Q[:,M_star:],mu= mean_value_quantizer(A_sorted[:,M_star:], np.around(Q_entry_list[0]))\n",
    "\n",
    "    inv_idx = np.argsort(idx_sorted)\n",
    "    Q = Q[:, inv_idx]\n",
    "    return Q, M_star, Q_entry_list\n",
    "\n",
    "class FWQ(nn.Module):\n",
    "    def __init__(self,token_dim,code_dim,discrete_size):\n",
    "        super(FWQ,self).__init__()\n",
    "        self.discrete_size=discrete_size\n",
    "    def forward(self,x,return_indice=False):\n",
    "        x_shape=x.shape\n",
    "        flattened_x=x.view(-1,x.shape[2]) #[B*S,H]\n",
    "        flattened_x_quantized,M_star, Q_entry_list=adaptive_featurewise_quantization(flattened_x,int(np.log2(self.discrete_size)))       \n",
    "        output=(torch.tensor(flattened_x_quantized).to(x.device)-flattened_x).detach()+flattened_x\n",
    "        L_comm=0\n",
    "        L_code=0\n",
    "        return output.reshape(x_shape),L_code,L_comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e5c15bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1566840/3267638797.py:51: RuntimeWarning: invalid value encountered in sqrt\n",
      "  v = (u * np.sqrt(81 - 12*u) + 9*u) ** (1/3)\n"
     ]
    }
   ],
   "source": [
    "X=np.random.randn(4,50)\n",
    "X_q,_,_=adaptive_featurewise_quantization(X,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c663bd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def generate_nf_table(bits: int, device=\"cpu\", dtype=torch.float16):\n",
    "    n_levels = 2 ** bits\n",
    "    # 在 (-1, 1) 的概率区间上均匀采样\n",
    "    probs = torch.linspace(0, 1, n_levels + 1)[1:-1]\n",
    "    # 根据标准正态分布的分位数函数生成\n",
    "    values = torch.erfinv(2 * probs - 1)\n",
    "    values = values / values.abs().max()  # 归一化到 [-1, 1]\n",
    "    values = torch.cat([torch.tensor([-1.0]), values, torch.tensor([1.0])])\n",
    "    return values.to(device=device, dtype=dtype)\n",
    "\n",
    "class NFNDoubleQuantizer(nn.Module):\n",
    "    def __init__(self, bits=4, block_size=64, use_double_quant=True):\n",
    "        super().__init__()\n",
    "        self.bits = bits\n",
    "        self.block_size = block_size\n",
    "        self.use_double_quant = use_double_quant\n",
    "        self.table=generate_nf_table(bits)\n",
    "\n",
    "    def quantize(self, x):\n",
    "        orig_shape = x.shape\n",
    "        x = x.view(orig_shape[0],orig_shape[1]//self.block_size, self.block_size)\n",
    "\n",
    "        q_idx_list, scale_list, min_list = [], [], []\n",
    "        x_min, x_max = x.min(dim=2).values.unsqueeze(-1), x.max(dim=2).values.unsqueeze(-1)\n",
    "        scales = (x_max - x_min).squeeze(-1)\n",
    "        x_norm = 2 * (x - x_min) / (x_max - x_min + 1e-8) - 1\n",
    "        dist = torch.abs(x_norm.unsqueeze(-1) - self.table.to(x.device))\n",
    "        q_idx = torch.argmin(dist, dim=-1).to(torch.uint8)\n",
    "        mins = x_min\n",
    "        # Double Quantization: 再量化scale (8bit)\n",
    "        if self.use_double_quant:\n",
    "            s_min, s_max = scales.min(dim=-1).values.unsqueeze(-1), scales.max(dim=-1).values.unsqueeze(-1)\n",
    "            scales_q = ((scales - s_min) / (s_max - s_min + 1e-8) * 255).round().to(torch.uint8)\n",
    "        else:\n",
    "            scales_q, s_min, s_max = None, None, None\n",
    "\n",
    "        return q_idx, scales_q, s_min, s_max, mins\n",
    "\n",
    "    def dequantize(self, q_idx, scales_q, s_min, s_max, mins):\n",
    "        if scales_q is not None:\n",
    "            scales = s_min + (scales_q.float() / 255) * (s_max - s_min)\n",
    "        else:\n",
    "            scales = s_min\n",
    "        scales=scales.unsqueeze(-1)\n",
    "        w_block = self.table[q_idx.to(device=self.table.device,dtype=torch.long)].to(torch.float32)\n",
    "        w_block = (w_block + 1) / 2 * scales +mins\n",
    " \n",
    "        return w_block.reshape(-1,q_idx.shape[1]*self.block_size)\n",
    "    \n",
    "class Qlora_quantize(nn.Module):\n",
    "    def __init__(self, bits=4, block_size=64, use_double_quant=True):\n",
    "        super().__init__()\n",
    "        self.quantizer = NFNDoubleQuantizer(bits, block_size, use_double_quant)\n",
    "\n",
    "    def forward(self, x):\n",
    "        flattened_x=x.view(-1,x.shape[2])\n",
    "        q_idx, s_q, s_min, s_max, mins = self.quantizer.quantize(flattened_x)\n",
    "        flattened_x_q = self.quantizer.dequantize(q_idx, s_q, s_min, s_max, mins)\n",
    "        output=flattened_x + (flattened_x_q - flattened_x).detach()\n",
    "        return output.reshape(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d97cea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([320, 1])\n"
     ]
    }
   ],
   "source": [
    "X=torch.rand(8,40,1280)\n",
    "quant=Qlora_quantize()\n",
    "q=quant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "08dfd576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8811, 0.1176, 0.6087,  ..., 0.9675, 0.6174, 0.4613],\n",
       "         [0.3412, 0.0083, 0.1451,  ..., 0.8400, 0.9867, 0.7271],\n",
       "         [0.2848, 0.7816, 0.8029,  ..., 0.0015, 0.7203, 0.9308],\n",
       "         ...,\n",
       "         [0.2455, 0.6643, 0.5318,  ..., 0.9995, 0.7583, 0.2779],\n",
       "         [0.6788, 0.6938, 0.4952,  ..., 0.5393, 0.1716, 0.7679],\n",
       "         [0.9702, 0.9675, 0.5222,  ..., 0.7753, 0.7366, 0.8458]],\n",
       "\n",
       "        [[0.8897, 0.0724, 0.7113,  ..., 0.2052, 0.6756, 0.8374],\n",
       "         [0.4419, 0.9382, 0.1345,  ..., 0.4369, 0.0828, 0.7452],\n",
       "         [0.7984, 0.5462, 0.6909,  ..., 0.4985, 0.4916, 0.0909],\n",
       "         ...,\n",
       "         [0.5281, 0.8096, 0.6119,  ..., 0.2170, 0.2455, 0.5626],\n",
       "         [0.5345, 0.1439, 0.9436,  ..., 0.8310, 0.2409, 0.2408],\n",
       "         [0.6460, 0.4772, 0.4199,  ..., 0.2782, 0.3231, 0.4355]],\n",
       "\n",
       "        [[0.8019, 0.5612, 0.5104,  ..., 0.7873, 0.2197, 0.2938],\n",
       "         [0.3004, 0.7838, 0.6811,  ..., 0.7415, 0.7341, 0.5568],\n",
       "         [0.3735, 0.6604, 0.7661,  ..., 0.9719, 0.7927, 0.1882],\n",
       "         ...,\n",
       "         [0.7787, 0.8121, 0.4211,  ..., 0.1291, 0.7970, 0.6240],\n",
       "         [0.5900, 0.4844, 0.0658,  ..., 0.5313, 0.1894, 0.4641],\n",
       "         [0.4280, 0.4834, 0.7724,  ..., 0.2925, 0.9725, 0.7745]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.4189, 0.9638, 0.8582,  ..., 0.4151, 0.6616, 0.3669],\n",
       "         [0.0576, 0.3921, 0.8314,  ..., 0.8862, 0.7353, 0.0068],\n",
       "         [0.6466, 0.4255, 0.9700,  ..., 0.6059, 0.7678, 0.8945],\n",
       "         ...,\n",
       "         [0.4480, 0.1644, 0.8236,  ..., 0.8485, 0.6373, 0.8835],\n",
       "         [0.9850, 0.5357, 0.5242,  ..., 0.1981, 0.6473, 0.0491],\n",
       "         [0.2846, 0.8219, 0.6398,  ..., 0.7833, 0.3936, 0.2741]],\n",
       "\n",
       "        [[0.9997, 0.4186, 0.0437,  ..., 0.4104, 0.6149, 0.1010],\n",
       "         [0.1193, 0.4960, 0.0487,  ..., 0.3686, 0.0152, 0.3806],\n",
       "         [0.0388, 0.3617, 0.6468,  ..., 0.8796, 0.1954, 0.3824],\n",
       "         ...,\n",
       "         [0.1765, 0.5833, 0.3777,  ..., 0.9172, 0.8414, 0.9530],\n",
       "         [0.2523, 0.7858, 0.9377,  ..., 0.0339, 0.7976, 0.2817],\n",
       "         [0.6548, 0.0433, 0.9323,  ..., 0.8961, 0.9434, 0.8465]],\n",
       "\n",
       "        [[0.5505, 0.0042, 0.3027,  ..., 0.7811, 0.9637, 0.4913],\n",
       "         [0.2690, 0.4944, 0.6303,  ..., 0.5090, 0.2124, 0.1532],\n",
       "         [0.6619, 0.6555, 0.9859,  ..., 0.2049, 0.0480, 0.4175],\n",
       "         ...,\n",
       "         [0.4287, 0.3703, 0.8015,  ..., 0.7740, 0.2197, 0.3343],\n",
       "         [0.9874, 0.1050, 0.9030,  ..., 0.3398, 0.8999, 0.8046],\n",
       "         [0.4606, 0.2549, 0.2896,  ..., 0.3017, 0.7237, 0.5861]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f256b0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinyllava",
   "language": "python",
   "name": "tinyllava"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
